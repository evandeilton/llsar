---
title: "llsar"
author: "José Lopes"
bibliography: referencias.bib
biblio-style: apalike
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{llsar}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error=F, message=F, warning=F, attr.source='.numberLines')
r = getOption("repos")
r["CRAN"] = "http://cran.us.r-project.org"
options(repos = r)

if(!require(schoolmath)){install.packages("schoolmath")}
if(!require(tidyverse)){install.packages("tidyverse")}
if(!require(knitr)){install.packages("knitr")}
if(!require(mvtnorm)){install.packages("mvtnorm")}
if(!require(numDeriv)){install.packages("numDeriv")}
if(!require(bbmle)){install.packages("bbmle")}
if(!require(broom)){install.packages("broom")}
if(!require(tidyverse)) {install.packages("tidyverse")}
if(!require(rgdal)) {install.packages("rgdal")}
if(!require(maptools)) {install.packages("maptools")}
if(!require(viridis)) {install.packages("viridis")}
if(!require(rgeos)){install.packages("rgeos")}
if(!require(gpclib)){install.packages("gpclib")}
if(!require(crossfire)){install.packages("crossfire")}
if(!require(spdep)){install.packages("spdep")}
if(!require(llsar)){devtools::install_github("evandeilton/llsar")}
```



# Contexto científico

A site [Fogo Cruzado](https://fogocruzado.org.br/) é uma plataforma digital colaborativa para monitoramento
civil da violência armada no Rio de Janeiro e no Recife. Usuários instalam um aplicativo no celular e
registram ocorrências de tiroteios (de maneira anônima). Os dados são disponibilizados publicamente através
de uma API, feito pela equipe do [`voltdatalab`](https://voltdata.info/). Depois de criar um cadastro no
[site](https://api.fogocruzado.org.br/) os dados de tiroteio podem ser extraídos diretamente a API com apoio 
do pacote `crossfire`.

```{r, echo=TRUE, eval=FALSE, error=F, message=F, warning=F}
# Carregamento e/ou instalação do pacote crossfire
if (!require("devtools")) install.packages("devtools")
if (!require("devtools")) devtools::install_github("voltdatalab/crossfire")

# Conectar com a API do voltdatalab
crossfire::fogocruzado_signin(email = "seu@email", password = "sua_senha")
```

# Objetivos

  - Obter os estimadores de máxima verossimilhança $\hat{\mu},\hat{\sigma}^2$ e $\hat{\rho}$ através do método de Newton-Rapson;
  - Obter as estimadores de máxima verossimilhança dos parâmetros ótimos por quasi-Newton `BFGS` e quasi-Newton com restrição `L-BFGS-B`;
  - Estudar a convergência de $\rho$ e dos outros parâmetros do modelo proposto;
  - Analisar graficatemente por mapa os valores preditos de `Y` pelo modelo;

> OBS.: $\hat{\rho}$ não pode ser muito próximo de 1, se existirem muitas adjacências, pois caso contrário a matriz $\textbf{I}-\rho\textbf{A}$ pode deixar de ser positiva definida.

# Dados e shapes

Para baixar os dados de registro de tiroteio dos últimos seis meses basta rodar o código a seguir.

```{r, echo=TRUE, eval=FALSE, error=F, message=F, warning=F}
# Cidades
cidades <- crossfire::get_cities()

# Ocorrências de tiroteio nos município da cidade do Rio de Janeiro
ocorrrj <- crossfire::get_fogocruzado(city = "Rio de Janeiro", 
                                      initial_date = Sys.Date() - months(6), 
                                      final_date = Sys.Date(),
                                      state = c("RJ"))
```

Os dados de shapes de todos os bairros do Rio de Janeiro (163 bairros, em 2020) para plotar os gráficos, podem ser baixados diteramente do [DataRio](https://www.data.rio/datasets/limite-de-bairros) depois importados pelo R com apoio dos pacotes `rgdal e sp`.

```{r, echo=TRUE, eval=FALSE, error=F, message=F, warning=F}
require(rgdal)
require(sp)
m <- rgdal::readOGR("dados/shp/Limite_de_Bairros.shp", 
                    verbose = FALSE, stringsAsFactors = FALSE, 
                    use_iconv = TRUE, encoding = "utf8") %>% 
  sp::spTransform(sp::CRS("+proj=longlat +ellps=WGS84 +datum=WGS84"))
rioGrid <- fortify(m, region = "CODBAIRRO_")
```

> OBS: Existe um problema entre os dados de tiroteio no rio e os dados de _shape_, pois o conjunto de dados de tiroteio não possui chave de bairro euquanto o arquivo de _shape_ tem, logo é preciso algum trabalho manual para ajustar as coisas. Outro fato é que os dados de tiroteio estão baseados nos município do censo de 2010, logo os municípios e "Bangu", "Gericinó", "Jabour" e "Vila Kennedy" foram removidos e restaram 159 bairros para dados e shapes. Para simplificar as coisas, colocamos os conjuntos de dados `rio_tiroteio` e `rio_shape` já trabalhados no pacote e eles podem ser baixados como segue:

```{r, echo=TRUE, eval=TRUE}
# Dados
data("rio_tiroteio", package = "llsar")

# Shapes
data("rio_shape", package = "llsar")

# A: Matriz de proximidades/adjacências
data("rio_proximidades", package = "llsar")
A <- rio_proximidades
```


```{r, fig.pos="h", fig.align='center', fig.height=4, fig.width=6, fig.cap="Mapa de ocorrências de tiroteio"}
rioGrid <- fortify(rio_shape, region = "CODBAIRRO_")
rioGrid %>%
  mutate(id = as.integer(id)) %>%
  left_join(rio_tiroteio, by = c("id" = "CODBAIRRO")) %>%
  ggplot(aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = count), col = "Black") +
  scale_fill_viridis() +
  theme_classic() +
  theme(legend.title = element_blank())
```



```{r, fig.pos="h", fig.align='center', fig.height=4,  fig.width=6, fig.cap="Matriz de adjacências entre bairros do Rio de Janeiro"}
op <- par(mar = c(5, 4, 4, 2) - 1.5)
rioPoly <- poly2nb(rio_shape, row.names = rio_shape@data$NOME)
rioNN <- nb2listw(rioPoly, style = "B", zero.policy = TRUE)
plot(rio_shape, main = "")
plot(rioNN, coordinates(rio_shape), col = "blue", add = TRUE)
```


# Modelo SAR

Essa classe de modelos faz parte dos modelos de regressão geoespaciais e são muito aplicados a fenômenos que se distibuiem espacialmente em uma região/mapa. Nesse estudo, a aplicação é a distribuição de tiroteio nos bairros da cidade do Rio de Janeiro no segundo semestre junho a novembro de 2020. Este é um exemplo didático, assim apesar dos dados não serem normais (para uma introdução veja Cressie, 1993, pág.
383) vamos trabalhar com o modelo SAR. Modelos SAR (_Spatial AutoRegressivo_) são frequentemente chamados de Spatial Lagged Model (_SLM_), mas a notação de ambos é igual.

Na literatura o modelo SAR pode ser escrito como $$ Y_i-\mu = \rho \sum_{j \in v(i)}(Y_j-\mu)+\epsilon$$ em que $v(i)$ é o conjunto de índices adjacentes à localização i; mas sem o próprio i; $0 \le \rho < c_0 < 1$ e $\epsilon \sim N(\textbf{0}, \sigma^2 \textbf{I}).$ Nesse caso, pode-se mostrar que o modelo é equivalente a $$(\textbf{I}-\rho\textbf{A})(\textbf{A}-\mu\textbf{1}) = \epsilon \ldots(1),$$ em que $\textbf{A}$ é a matriz de adjacências. Consequentemente, se o modelo SAR for simples, isto é, sem covariáveis $E(\textbf{Y}) = \mu\textbf{1}$ e Var$(\textbf{Y}) = \sigma^2(\textbf{I}-\rho \textbf{A})^{-2}.$ onde $\mu$ é uma contante multiplicada por um vetor constante de escalares, digamos $\textbf{1}s$. Quando o modelo possui preditoras $E(\textbf{Y}) = \mu = \textbf{X} \beta$. Com base em (1), é possível construir a função de verossimilhança do modelo e a implementação do Método de Newton-Rapson. 

  - $\textbf{Y}$ é um vetor $n × 1$ da variável dependente;
  - $\textbf{A}$ é a matroz de ajdacências ou pesos espaciais $n × n$;
  - $\textbf{X}$ é uma matriz $n × k$ que é a metrix de variáveis independentes (marix de delineamento experimental);
  - $\beta$ é o vetor $k x 1$ de parâmetros do modelo;
  - $\rho$ é o prâmetro que mde o grau de correlação espacial entres os bairros;
  - $\epsilon$ é o vetor $n × 1$ de termos de erro do modelo.

O modelo sugerido tem a forma $$\begin{aligned}
Y_i-\mu = \rho \sum_{j \in v(i)}(Y_j-\mu)+\epsilon \\
(Y_i-\mu) - \rho \sum_{j \in v(i)}(Y_j-\mu) = \epsilon
\end{aligned}$$ e pode ser reescrito de forma matricial de modo que $$\begin{aligned} \epsilon
&= (\textbf{Y}-\mu\textbf{1}) - \rho \textbf{A}(\textbf{Y}-\mu\textbf{1}) \\ 
&= (\textbf{I}-\rho\textbf{A})(\textbf{Y}-\mu\textbf{1})
\end{aligned}$$ e como dado por definição do problema $\textbf{Y} \sim \textbf{N}_{mv} (\mu\textbf{1}, \sigma^2(\textbf{I}-\rho \textbf{A})^{-2}).$ Com isso a meta é estimar, através do método de máxima verossimilhança, os parâmetros do modelo dado com maximização através do método de Newton-Rapson. Para tanto, determina-se três funções principais:

  - $l(\theta)$ que é o log da função de verossomilhança $L(\theta)$ para uma realização de uma normal multivariada com média $E(\textbf{Y}) = \mu\textbf{1}$ e variância $Var(\textbf{Y}) = \sigma^2(\textbf{I}-\rho \textbf{A})^{-2}$
  - Vetor gradiente de $l(\theta)$ e 
  - Matriz hessiana de $l(\theta)$, onde $\theta = (\beta^{\top}, \sigma^2, \rho)$.
  
Além disso, precisamos programar a função dos preditos do modelo final e realizar os estudos necessários. Os preditos foram definidos a partir de duas visões, a primeira é direta pela definição do problema, pois tem-se que $E(Y) = \mu$ e a segunda, dada pela nova definição chamada de _out-of-sample_ (oot), é dada pela expressão $\hat{Y_i} = \hat{\mu} + \hat{\Sigma}_{(i,-i)}\hat{\Sigma}_{(-i,-i)}^{-1}(Y_{-i}-\hat{\mu}\mathbf{1}),$ onde os indices $(i=i,j=-i)$ e $(i = -i,j = -i)$ indicam respectivamente só linha $i$ sem coluna $i$ e sem linha $i$ nem coluna $i$. Em $Y_{-i}$ indica sem a observação $i$.

# Implementação

Parâmetros de variância, correlação e escala costumam ser problemáticos para convergir e nesse modelo o $\rho$ é extremamente sensível e difícil de convergir, especialmente quando se aproxima de um, pois a matriz de adjacências se torna não positiva definida. Houve um forte desafio em derivar analiticamente as expressões tanto do vetor gradiente com as derivadas de ordem um, como as derivadas de ordem dois para a Hessiana. Contudo, foi implementado duas formas de solução: (1) por derivadas aproximadas numericamente e (2) por derivação analítica. Decidiu-se usar a solução analítica por ser menos custosa computacionalnete, mas ambas as abordagens tiveram uma precisão da ordem de 6 casas decimais. Para a solução por aproximação numérica, o pacote `numDeriv` [@numDeriv] se mostrou uma opção viável para problema de dificil solução analítica, uma vez que permite determinar com grande precisão o gradiente e a matriz hessiana aproximados numericamente para funções multidimencionais. O método presente nesse pacote é o de Richardson [@fornberg1981numerical] e pode ser aplicado diretamente na log-verossimilhança construída de forma adequada. Segundo a nossa proposta de solução, foi possível obter a convergência do método de Newton com poucas iterações e com boa precisão, seja por aproximação numérica ou algébrica, desde que passemos bons chutes iniciais. No Apêndice está o código documentado e a seguir detalhamos o passo a passo de nossas propostas de solução. Quanto à solução analítica, com apoio da ferramenta de derivação matricial [MatrixCalculus](http://www.matrixcalculus.org/matrixCalculus) mais trabalho adicional nos casos mais delicados, foi possível obter as derivadas corretas, o que tornou a programação em R mais rápida.

## Solução numérica

Para reduzir o espaço de busca e aumentar a velocidade de convergência é importante começar com a função de log-verossimilhança bem construída e bons chutes iniciais. Nossa proposta de solução passa pelos os seguintes passos:

  - Determinar algebricamente a função de log-verossimilhança de $L(\theta);$ $$\begin{aligned}f(y;\theta) &=  \frac{1}{(2\pi)^{n/2})|\Sigma|^{1/2}}\mathrm{exp \left \{ -\frac{1}{2}(\mathbf{y}-\mathbf{\mu}) \Sigma^{-1}(\mathbf{y}-\mathbf{\mu})\right \}}\end{aligned}.$$ Fazendo $f(y;\theta) = L(\theta)$ temos que $$\begin{aligned}l(\theta) 
&= \log{L(\theta)} \\
&= \log \left (\frac{1}{(2\pi)^{n/2})|\Sigma|^{1/2}}\mathrm{exp \left \{ -\frac{1}{2}(\mathbf{y}-\mathbf{\mu}) \Sigma^{-1}(\mathbf{y}-\mathbf{\mu})\right \}} \right ) \\
&=\frac{-\log{2\pi}}{2}-\frac{n\log{\sigma^2}}{2}+\frac{1}{2\sigma^2}\log{|(\mathbf{I}-\rho \mathbf{A})^2|}+\\
& -\frac{1}{2\sigma^2}(\mathbf{y}-\rho \mathbf{\mu})^{\top}(\mathbf{I}-\rho \mathbf{A})^2(\mathbf{y}-\rho \mathbf{\mu})
\end{aligned},$$ onde foram feitas em $l(\theta)$ as substituições $$\begin{aligned}
\Sigma^{-1} &= (\sigma^2(\mathbf{I}-\rho \mathbf{A})^{-2})^{-1} = \sigma^{-2}(\mathbf{I}-\rho \mathbf{A})^{\top}(\mathbf{I}-\rho \mathbf{A})
\end{aligned}$$ e $$\begin{aligned}|\Sigma| 
&= |\sigma^2(\mathbf{I}-\rho \mathbf{A})^{-2}| = \sigma^{2n}|(\mathbf{I}-\rho \mathbf{A})^{-2}|
\end{aligned}$$
  
  - Determinar numericamente o vetor gradiente que é dado por $$\mathbf{\nabla}(\theta) = l(\theta)' = \left (\frac{\partial l(\theta)}{\partial \beta}, \frac{\partial l(\theta)}{\partial \sigma^2}, \frac{\partial l(\theta)}{\partial \rho} \right ),$$ com apoio da função `numDeriv::grad()`.
  
  - Determinar numericamente a matriz hessiana definida por $$\mathbf{H(\theta) = \begin{pmatrix} 
  \frac{\partial l(\theta)}{\partial \mu \partial \mu} & 
  \frac{\partial l(\theta)}{\partial \mu \partial \sigma^2} &
  \frac{\partial l(\theta)}{\partial \mu \partial \rho}  \\ 
  \frac{\partial l(\theta)}{\partial \sigma^2 \partial \mu} &
  \frac{\partial l(\theta)}{\partial \sigma^2 \partial \sigma^2} &
  \frac{\partial l(\theta)}{\partial \sigma^2 \partial \rho} \\ 
  \frac{\partial l(\theta)}{\partial \rho \partial \mu} &
  \frac{\partial l(\theta)}{\partial \rho \partial \sigma^2} &
  \frac{\partial l(\theta)}{\partial \rho \partial \rho}
  \end{pmatrix}},$$ utilizando a função `numDeriv::hessian()`
  
  - Utilizar a função `lm()` para determinar chutes iniciais para $\mathbf{x_{0}} = (\hat \mu_0, \hat \sigma_0, \hat \rho_0)$ e fazendo $\hat \rho_0 = \frac{1}{\sigma_0^2}$ através das estimativas de mínimos quadrados;
  
  - Aplicar o algoritmo de Newton $\mathbf{x_{k+1}} = \mathbf{x_{k}}-\mathbf{H^{-1}(\theta)}\nabla{(\theta)}$ com os chutes iniciais e rodar até atingur a convergência quando o valor de $tol$ é tal que $tol > \left( \sum \nabla{(\theta)})\right)^{2}$

## Solução analítica

O procedimento de ajuste por Newton programada é flexível para ajustar modelos pelos dois métodos discutidos aqui: o método de derivação por aproximação numérica e o método derivação analítica. Para trabalhar analiticamente, segue-se o passo a passo de uma das soluções propostas substituindo as funções corretas. Omitiu-se as fórmulas da solução analítica neste relatório, mas elas estão programadas nas rotinas do Apêndice.

# Resultados

## Estimativas modelo sem preditoras

```{r, echo=F, eval=T}
a1 <- fit_newton(lvero = sar_reg_lvero_analitico, 
                 gr = sar_reg_gradiente_analitico, 
                 hess = sar_reg_hessiano_analitico,
                 formula = count ~ 1, dados = rio_tiroteio, A = A, verbose = F)
fit1 <- fit_quasi_newton(a1)
fit1_profile <- bbmle::profile(fit1)
```

Utilizando como chutes iniciais os valores presentes na Tabela 2, obtidos através da função `fit_newton()`, estimou-se $c_0$ que é o máximo obtido por $\rho$ foi igual a `r fit1_profile@profile$rho %>% as.matrix() %>% as.data.frame() %>% dplyr::select(ncol(.)) %>% pull() %>% max()` com `r a1$processamento %>% nrow()` iterações. O código a seguir mostra como foram obtidos os resultados atráves de nossa solução implementada.

```{r, echo=T, eval=F}
a1 <- fit_newton(lvero = sar_reg_lvero_analitico, 
                 gr = sar_reg_gradiente_analitico, 
                 hess = sar_reg_hessiano_analitico,
                 formula = count ~ 1, dados = rio_tiroteio, A = A, verbose = F)
fit1 <- fit_quasi_newton(a1)
fit1_profile <- bbmle::profile(fit1)
```


```{r, fig.pos="h", fig.align='center'}
a1$processamento %>% 
  dplyr::mutate(niteracoes = n()) %>% 
    dplyr::select(-crit) %>% dplyr::top_n(1, loglik) %>% 
  knitr::kable(digits = 4, 
               caption = "Chutes iniciais",
               col.names = c("mu","sigma","rho","loglik","niteracoes"),
               row.names = F)
```

Embora tendo que $\rho$ foi estimado com valor negativo e estatísticamente igual a zero, [@ord1975estimation] argumenta que sob algumas condições dos pesos, tem-se que $\rho < 1$. Além disso, se consideramos o módulo de $|\rho|$ teremos uma estimativa que atende o critério dado nesse problema. O perfil de verossimilhança na Figura 1 mostra que no range de variação, ele pode ser qualquer valor entre zero e 0.06 com 99% de confiança. Do mesmo modo as outras estimativas estão contidas em intervalos bem definidos e sem sinais de alteração em virtude de mínimos locais.
  
```{r, fig.pos="h", fig.height=6, fig.width=8, fig.align='center', fig.cap="Perfis de verossimilhança"}
par(mfrow = c(2,2)) 
plot(fit1_profile)
```

Por fim a Tabela 3 mostra as estimativas dos parâmetros e todos, exceto $\rho$ possuem forte significância com p.valor próximo de zero com intervalo de confiança de 95%.

```{r, fig.pos="h", fig.align='center'}
tidy(fit1) %>% 
  bind_cols(
    confint(fit1, method = "quad") %>% 
      as.data.frame()
  ) %>% 
  knitr::kable(digits = 4, 
               caption = "MLE modelo sem covariáveis", 
               row.names = F,
               col.names = c("term", "estimate", "se", "z.value", "p.value", "ci2.5","ci97.5")) 
# %>% 
#   kableExtra::kable_styling(font_size = tfonte)
```

## Estimativas modelo com preditoras

Também foi analisado o modelo SAR com o logaritmo das covariáveis Inhabitants e Area explicando a contagem de tiroteios (count), porém apenas a variável `loginhabitants` teve estimativa significativa. O código abaixo mostra como a estimação foi feita.

```{r, echo=T, eval=T}
a2 <- fit_newton(lvero = sar_reg_lvero_analitico, 
                 gr = sar_reg_gradiente_analitico, 
                 hess = sar_reg_hessiano_analitico,
                 formula = count ~ loginhabitants, dados = rio_tiroteio, A = A, verbose = F)
fit2 <- fit_quasi_newton(a2)
fit2_profile <- bbmle::profile(fit2)
```

Semelhante ao primeiro caso, e utilizando como chutes iniciais os valores presentes na Tabela 4, $c_0$ que é o máximo obtido por $\rho$ foi igual a `r fit2_profile@profile$rho %>% as.matrix() %>% as.data.frame() %>% dplyr::select(ncol(.)) %>% pull() %>% max()` com `r a2$processamento %>% nrow()` iterações.
```{r, fig.pos="h", fig.align='center'}
a2$processamento %>% 
  dplyr::mutate(niteracoes = n()) %>% 
    dplyr::select(-crit) %>% dplyr::top_n(1, loglik) %>% 
  knitr::kable(digits = 4, 
               caption = "Chutes iniciais",
               col.names = c("intercepto","loginhabitants","sigma","rho","loglik","niteracoes"),
               row.names = F) 
```
Assim, na Tabela 5 estão as estimativas dos parâmetros. Novamente o parâmetro $\rho$ teve sua estimativa negativa e estatísticamente nula com p.valor de $-0.0071 (0.0244)$. Como esperado, o parâmetro $sigma$ na presença da covariável logihabitants teve uma queda em relação ao modelo simples e o intercepto mudou significativamente. É importante notar também que o valor ótimo da log-vero ficou menor, também esperado uma vez que a preditora agrega mais informação ao modelo.
```{r, fig.pos="h", fig.align='center'}
tidy(fit2) %>% 
  bind_cols(
    confint(fit2, method = "quad") %>% 
      as.data.frame()
  ) %>% 
  knitr::kable(digits = 4, 
               caption = "MLE modelo count ~ loginhabitants", 
               row.names = F,
               col.names = c("term", "estimate", "se", "z.value", "p.value", "ci2.5","ci97.5")) 
```

Por fim, analisamos os perfis de verossimilhança afim de entender melhor o comportamento dos prâmetros próximo à região e convergência.
```{r, fig.pos="h", fig.align='center', fig.height=6, fig.width=8, fig.cap="Perfis de verossimilhança"}
plot(fit2_profile)
```
A Figura 2 mostra que houve oscilação na convergência do intercepto e também da variável loginhabitants na vizinhança do ótimo. Como esperado $\sigma$ apresenta assimetria, porém sem perda de estabilidade na vizinhança do ótimo. Quanto a $\rho$ vemos que ele possui boa estabilidade em torno de zero na vizinhança, mas podendo estar em $(0, 0.006)$.

## Valores preditos e gráfico

Na Tabela 6 tem-se que em 20% das unidades amostradas houve entre 27 e 101 registros de tiroteio na amostra observada. Já para os preditos, temos que no modelo simples, a valor predito ficou entre 9 e 10 para todos os municípios. Já para o modelo com a variável loginhabitants os preditos foram mais realistas.

```{r, eval=T, echo=T}
predict_simples     <- sar_reg_predict_oot(fit1) %>% rename("pred_s" = pred)
predict_covariaveis <- sar_reg_predict_oot(fit2) %>% rename("pred_c" = pred)
```


```{r}
simples <- sar_reg_predict_oot(fit1) %>% rename("pred_s" = pred)
covaria <- sar_reg_predict_oot(fit2) %>% rename("pred_c" = pred)
predis  <- rio_tiroteio %>% 
  dplyr::select(id = CODBAIRRO, bairro = neighborhood, count) %>% 
  dplyr::inner_join(simples, by = "id") %>% 
  dplyr::inner_join(covaria, by = "id") %>% 
  dplyr::arrange(desc(count))

bind_cols(medida = c("observado","predsimples","predcovariavel"),
          bind_rows(rio_tiroteio %>% pull(count) %>% quantile(probs = seq(0, 1, 0.10)),
                    simples %>% pull(pred_s) %>% quantile(probs = seq(0, 1, 0.10)),
                    covaria %>% pull(pred_c) %>% quantile(probs = seq(0, 1, 0.10))) %>% 
            round() %>% as.data.frame()) %>% 
  knitr::kable(caption = "Decis da variável resposta count.", 
               row.names = FALSE)
# %>% 
#   kableExtra::kable_styling(font_size = tfonte)
```

Os top cinco municípios com mais ocorrências de acordo a base são: Cidade de Deus, Complexo do Alemão, Tijuca, Vicente de Carvalho e Cordovil. Destes, o modelo predito com uma variável apontou somente Tijuca, porém vale salientar aqui que mesmo com apenas uma covariável (loglhabitantes) a correlação entre o observado e os preditos saiu de 0.008 para 0.3974 com a adição de apenas uma variável explicativa. 

```{r, fig.pos="h", fig.align='center', fig.height=8, fig.width=8, fig.cap="Perfis de verossimilhança"}
rioGrid <- try(fortify(rio_shape, region = "CODBAIRRO_"))

da <- rioGrid %>%
  dplyr::mutate(id = as.integer(id)) %>%
  dplyr::left_join(predis, by = "id")

g1 <- da %>% 
  ggplot(aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = count), col = "Black") +
  scale_fill_viridis() +
  theme_classic() +
  theme(legend.position = "bottom")+
  labs(title = "(A) Observado")

g2 <- da %>% 
  ggplot(aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = pred_s), col = "Black") +
  scale_fill_viridis() +
  theme_classic() +
  theme(legend.position = "bottom")+
  labs(title = "(B) Predito simples")

g3 <- da %>% 
  ggplot(aes(x = long, y = lat, group = group)) +
  geom_polygon(aes(fill = pred_c), col = "Black") +
  scale_fill_viridis() +
  theme_classic() +
  theme(legend.position = "bottom")+
  labs(title = "(C) Predito com covariável")
gridExtra::grid.arrange(g1, g2, g3, ncol = 2)
``` 
 
Observando a Figura 3 nota-se que o modelo com uma covariável (C) acerta mais para baixas contagens e o modelo sem covariável (B), como esperado faz uma média geral por município. Os dados são bastantes assimetricos então talvez uma estrutura que contemple melhor essa relação possa trazer maior ganho.

# Discussão

Este trabalho foi bastante desafiador e está longe de uma solução definitiva para o modelo SAR sugerido. Um dos principais motivos é que a estimativa de $\rho$ dentro de seu espaço paramétrico é muito problemática em vários aspectos, entre eles o risco de fuga de $\rho$ do intervalo $(0,1)$ dado no problema e isso ocorreu nas estimativas. Além disso, este parâmetro afeta a matriz de ajacências podendo prejudicar todo o modelo tornando matrizes não inversíveis. Os parâmetros estimados pelo método de máxima verossimilhança através da maximização com o método de Newton foi rápida, convergindo em menos de 10 passos, a partir do momento em que tivemos as funções de gradiente e hessiana bem programadas. Seja via derivação numérica ou analítica, sendo que esta última, conforme esperado, foi mais rápida. Os chutes iniciais através dos estimadores de mínimos quadrados foram muito importantes para atingir convergência e garantir um mínimo adequado para todos os parâmetros. Quanto aos preditos, do ponto de vista do modelo sem covariável, vale o exercício, mas não tem sentido prático, uma vez que a quantidade de crimes não se distribui de forma constante nos municípios. Já o modelo com uma covariável (log-ihabitantes) mostrou estimativas estatísticamente razoáveis para a preditora, mesmo numa regressão linear, que como sabido, não é a melhor abordagem para contagens com forte assimetria, como é o nosso caso.

# Referências
<div id="refs"></div>

